{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3 as sqlite\n",
    "con = sqlite.connect('test.sqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(\"SELECT * from Documents\", con)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ED_ENC_NUM</th>\n",
       "      <th>NOTE_TEXT</th>\n",
       "      <th>Score</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12529</th>\n",
       "      <td>12341</td>\n",
       "      <td>\\nBELGIUM CUTS TREASURY CERTIFICATE RATES    BRUSSELS, April 3 - The Belgian National Bank cut interest\\nrates on one, two and three-month treasury certificates to 7.30\\npct from 7.40 pct effectiv...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19625</th>\n",
       "      <td>19626</td>\n",
       "      <td>\\nAVON &lt;AVP&gt; SEES HIGHER 4TH QTR, 1987 EARNINGS    NEW YORK, Oct 20 - Avon Products Inc, which earlier\\nreported lower third quarter profits, said its fourth quarter\\nand full year earnings will e...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4522</th>\n",
       "      <td>4145</td>\n",
       "      <td>\\nLAMSON AND SESSIONS CO &lt;LMS&gt; 4TH QTR LOSS    NEW YORK, March 11 -\\n    Oper shr loss 26 cts vs profit five cts\\n    Oper net loss 1,506,000 vs profit 312,000\\n    Revs 42 mln vs 27.9 mln\\n    Ye...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>573</td>\n",
       "      <td>\\nALCAN ALUMINIUM LTD &lt;AL&gt; SETS STOCK SPLIT    MONTREAL, March 2 - Alcan Aluminium Ltd said its board\\ndeclared a three-for-two stock split, subject to shareholder\\napproval at the April 23 annual...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17891</th>\n",
       "      <td>17829</td>\n",
       "      <td>\\nMITSUBISHI HEAVY BUILDS ENERGY-SAVING TANKER    TOKYO, June 16 - Mitsubishi Heavy Industries Ltd &lt;MITH.T&gt;\\nsaid it began building the world's most advanced energy-saving\\ntanker, which consumes ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12285</th>\n",
       "      <td>12097</td>\n",
       "      <td>\\nBRITISH TELECOM TO SELL EQUIPMENT IN NORTH AMERICA    LONDON, April 2 - British Telecommunications Plc &lt;BTY.L&gt;\\nwill market electronic data transmission equipment of its own\\ndesign in North Ame...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9026</th>\n",
       "      <td>8838</td>\n",
       "      <td>\\nNASD PRESIDENT LEAVES FOR HAMBRECHT AND QUIST    NEW YORK, March 24 - &lt;Hambrecht and Quist Group&gt; said\\nGordon Macklin has resigned as president of the &lt;National\\nAssociation of Securities Deale...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12359</th>\n",
       "      <td>12171</td>\n",
       "      <td>\\nNO EXTENSION ON U.S. DAIRY HERD BUYOUT - LYNG    WASHINGTON, April 2 - U.S. Agriculture Secretary Richard\\nLyng said he would not agree to an extension of the 18-month\\nwhole dairy herd buyout p...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3316</th>\n",
       "      <td>2939</td>\n",
       "      <td>\\nGERMAN SECURITIES PURCHASES SET RECORD IN JANUARY    FRANKFURT, March 6 - Purchases of West German bonds in\\nJanuary reached a record 13 billion marks' worth as a result of\\ninvestment from othe...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14581</th>\n",
       "      <td>14393</td>\n",
       "      <td>\\nJAPAN, BRITAIN TO EXCHANGE SECURITIES MARKET INFO    TOKYO, April 8 - Japan and Britain have signed a memorandum\\non exchange of securities market information to protect\\ninvestors and promote t...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ED_ENC_NUM  \\\n",
       "12529       12341   \n",
       "19625       19626   \n",
       "4522         4145   \n",
       "572           573   \n",
       "17891       17829   \n",
       "12285       12097   \n",
       "9026         8838   \n",
       "12359       12171   \n",
       "3316         2939   \n",
       "14581       14393   \n",
       "\n",
       "                                                                                                                                                                                                     NOTE_TEXT  \\\n",
       "12529  \\nBELGIUM CUTS TREASURY CERTIFICATE RATES    BRUSSELS, April 3 - The Belgian National Bank cut interest\\nrates on one, two and three-month treasury certificates to 7.30\\npct from 7.40 pct effectiv...   \n",
       "19625  \\nAVON <AVP> SEES HIGHER 4TH QTR, 1987 EARNINGS    NEW YORK, Oct 20 - Avon Products Inc, which earlier\\nreported lower third quarter profits, said its fourth quarter\\nand full year earnings will e...   \n",
       "4522   \\nLAMSON AND SESSIONS CO <LMS> 4TH QTR LOSS    NEW YORK, March 11 -\\n    Oper shr loss 26 cts vs profit five cts\\n    Oper net loss 1,506,000 vs profit 312,000\\n    Revs 42 mln vs 27.9 mln\\n    Ye...   \n",
       "572    \\nALCAN ALUMINIUM LTD <AL> SETS STOCK SPLIT    MONTREAL, March 2 - Alcan Aluminium Ltd said its board\\ndeclared a three-for-two stock split, subject to shareholder\\napproval at the April 23 annual...   \n",
       "17891  \\nMITSUBISHI HEAVY BUILDS ENERGY-SAVING TANKER    TOKYO, June 16 - Mitsubishi Heavy Industries Ltd <MITH.T>\\nsaid it began building the world's most advanced energy-saving\\ntanker, which consumes ...   \n",
       "12285  \\nBRITISH TELECOM TO SELL EQUIPMENT IN NORTH AMERICA    LONDON, April 2 - British Telecommunications Plc <BTY.L>\\nwill market electronic data transmission equipment of its own\\ndesign in North Ame...   \n",
       "9026   \\nNASD PRESIDENT LEAVES FOR HAMBRECHT AND QUIST    NEW YORK, March 24 - <Hambrecht and Quist Group> said\\nGordon Macklin has resigned as president of the <National\\nAssociation of Securities Deale...   \n",
       "12359  \\nNO EXTENSION ON U.S. DAIRY HERD BUYOUT - LYNG    WASHINGTON, April 2 - U.S. Agriculture Secretary Richard\\nLyng said he would not agree to an extension of the 18-month\\nwhole dairy herd buyout p...   \n",
       "3316   \\nGERMAN SECURITIES PURCHASES SET RECORD IN JANUARY    FRANKFURT, March 6 - Purchases of West German bonds in\\nJanuary reached a record 13 billion marks' worth as a result of\\ninvestment from othe...   \n",
       "14581  \\nJAPAN, BRITAIN TO EXCHANGE SECURITIES MARKET INFO    TOKYO, April 8 - Japan and Britain have signed a memorandum\\non exchange of securities market information to protect\\ninvestors and promote t...   \n",
       "\n",
       "       Score  Category  \n",
       "12529      0       NaN  \n",
       "19625      0       NaN  \n",
       "4522       0       NaN  \n",
       "572        0       NaN  \n",
       "17891      0       NaN  \n",
       "12285      0       NaN  \n",
       "9026       0       NaN  \n",
       "12359      0       NaN  \n",
       "3316       0       NaN  \n",
       "14581      0       NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 200\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ED_ENC_NUM    20791\n",
       "NOTE_TEXT     20791\n",
       "Score         20791\n",
       "Category          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.columns\n",
    "df_unlabeled = df[(df[u'Category'].isnull())]\n",
    "df_unlabeled.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ED_ENC_NUM    80\n",
       "NOTE_TEXT     80\n",
       "Score         80\n",
       "Category      80\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled = df[(df[u'Category'] == 1) | (df[u'Category'] == 2)]\n",
    "df_labeled.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda\\lib\\site-packages\\IPython\\kernel\\__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from IPython.kernel.zmq import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "def ConvertCategoryColToBinVal(df, true_val):\n",
    "    df_labeled['Category'] = df_labeled['Category'].apply(lambda x: x == true_val)\n",
    "    \n",
    "ConvertCategoryColToBinVal(df_labeled, 1)\n",
    "#df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.axis('equal')\n",
    "plt.pie(\n",
    "    df_labeled.Category.value_counts().tolist(), \n",
    "    labels=['False', 'True'], \n",
    "    autopct='%1.1f%%', \n",
    "    colors=(\"#E13F29\", \"#D69A80\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\user-\n",
      "[nltk_data]     old\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [item for item in tokens if item.isalpha()]\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64L,) (64L,)\n",
      "(16L,) (16L,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_labeled['NOTE_TEXT'], df_labeled['Category'], test_size=0.2)\n",
    "print X_train.shape, y_train.shape\n",
    "print X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=tokenize, ngram_range=(1,2))\n",
    "\n",
    "wcounts = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<64x12760 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 20355 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'a',\n",
       " u'a billion',\n",
       " u'a bioengin',\n",
       " u'a biotech',\n",
       " u'a biotechnolog',\n",
       " u'a bizarr',\n",
       " u'a boston',\n",
       " u'a broadbas',\n",
       " u'a build',\n",
       " u'a bullish',\n",
       " u'a bundesbank',\n",
       " u'a call',\n",
       " u'a candid',\n",
       " u'a caretak',\n",
       " u'a chanc',\n",
       " u'a clear',\n",
       " u'a clinic',\n",
       " u'a clone',\n",
       " u'a close',\n",
       " u'a combin',\n",
       " u'a comment',\n",
       " u'a common',\n",
       " u'a compani',\n",
       " u'a competit',\n",
       " u'a complic',\n",
       " u'a compromis',\n",
       " u'a concern',\n",
       " u'a condit',\n",
       " u'a consensu',\n",
       " u'a consolid',\n",
       " u'a control',\n",
       " u'a correspond',\n",
       " u'a crosscurr',\n",
       " u'a cure',\n",
       " u'a cut',\n",
       " u'a day',\n",
       " u'a decis',\n",
       " u'a definit',\n",
       " u'a develop',\n",
       " u'a differ',\n",
       " u'a difficult',\n",
       " u'a direct',\n",
       " u'a discoveri',\n",
       " u'a distribut',\n",
       " u'a diuret',\n",
       " u'a dosag',\n",
       " u'a dozen',\n",
       " u'a draft',\n",
       " u'a drop',\n",
       " u'a drought',\n",
       " u'a drug',\n",
       " u'a drugfre',\n",
       " u'a faithful',\n",
       " u'a fatal',\n",
       " u'a fee',\n",
       " u'a few',\n",
       " u'a file',\n",
       " u'a final',\n",
       " u'a firm',\n",
       " u'a fiscal',\n",
       " u'a five',\n",
       " u'a fix',\n",
       " u'a flurri',\n",
       " u'a foreign',\n",
       " u'a formal',\n",
       " u'a former',\n",
       " u'a frequent',\n",
       " u'a friendli',\n",
       " u'a full',\n",
       " u'a further',\n",
       " u'a genentech',\n",
       " u'a genet',\n",
       " u'a german',\n",
       " u'a glass',\n",
       " u'a good',\n",
       " u'a healthier',\n",
       " u'a heart',\n",
       " u'a hepat',\n",
       " u'a hereditari',\n",
       " u'a high',\n",
       " u'a highend',\n",
       " u'a histori',\n",
       " u'a host',\n",
       " u'a hous',\n",
       " u'a huge',\n",
       " u'a hybrid',\n",
       " u'a hyoallergen',\n",
       " u'a joint',\n",
       " u'a judgement',\n",
       " u'a juri',\n",
       " u'a key',\n",
       " u'a kuwaiti',\n",
       " u'a languag',\n",
       " u'a larg',\n",
       " u'a law',\n",
       " u'a legal',\n",
       " u'a legitim',\n",
       " u'a line',\n",
       " u'a littl',\n",
       " u'a live',\n",
       " u'a longdat',\n",
       " u'a lot',\n",
       " u'a low',\n",
       " u'a lower',\n",
       " u'a major',\n",
       " u'a male',\n",
       " u'a man',\n",
       " u'a market',\n",
       " u'a match',\n",
       " u'a mechan',\n",
       " u'a meeet',\n",
       " u'a meet',\n",
       " u'a milder',\n",
       " u'a million',\n",
       " u'a minimum',\n",
       " u'a minut',\n",
       " u'a mistrial',\n",
       " u'a mln',\n",
       " u'a more',\n",
       " u'a move',\n",
       " u'a net',\n",
       " u'a netherland',\n",
       " u'a new',\n",
       " u'a news',\n",
       " u'a nonexclus',\n",
       " u'a number',\n",
       " u'a onceaday',\n",
       " u'a oneforf',\n",
       " u'a panel',\n",
       " u'a par',\n",
       " u'a parti',\n",
       " u'a patent',\n",
       " u'a patient',\n",
       " u'a pct',\n",
       " u'a period',\n",
       " u'a person',\n",
       " u'a pharmaceut',\n",
       " u'a physician',\n",
       " u'a pipelin',\n",
       " u'a placebo',\n",
       " u'a plan',\n",
       " u'a possibl',\n",
       " u'a potenti',\n",
       " u'a potentiallyfat',\n",
       " u'a poxlik',\n",
       " u'a precursor',\n",
       " u'a press',\n",
       " u'a pretax',\n",
       " u'a privat',\n",
       " u'a problem',\n",
       " u'a product',\n",
       " u'a prolong',\n",
       " u'a protocol',\n",
       " u'a purchas',\n",
       " u'a quarter',\n",
       " u'a rare',\n",
       " u'a recombin',\n",
       " u'a recommend',\n",
       " u'a record',\n",
       " u'a rehear',\n",
       " u'a renew',\n",
       " u'a report',\n",
       " u'a research',\n",
       " u'a respons',\n",
       " u'a result',\n",
       " u'a resumpt',\n",
       " u'a review',\n",
       " u'a revis',\n",
       " u'a right',\n",
       " u'a rock',\n",
       " u'a safeti',\n",
       " u'a sandlik',\n",
       " u'a savior',\n",
       " u'a schedul',\n",
       " u'a scientist',\n",
       " u'a second',\n",
       " u'a sellout',\n",
       " u'a senior',\n",
       " u'a seri',\n",
       " u'a seriou',\n",
       " u'a sevenyear',\n",
       " u'a share',\n",
       " u'a sharp',\n",
       " u'a short',\n",
       " u'a sign',\n",
       " u'a signific',\n",
       " u'a similar',\n",
       " u'a small',\n",
       " u'a smallscal',\n",
       " u'a smithklin',\n",
       " u'a solvent',\n",
       " u'a sourc',\n",
       " u'a south',\n",
       " u'a special',\n",
       " u'a specif',\n",
       " u'a spokesman',\n",
       " u'a spokeswoman',\n",
       " u'a startl',\n",
       " u'a state',\n",
       " u'a statement']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = vectorizer.get_feature_names()\n",
    "\n",
    "feats[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(wcounts, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AUC: ', 1.0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions = model.predict(vectorizer.transform(X_test))\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12760\n",
      "12760\n",
      "Smallest Coefs:\n",
      "[u'blah' u'blah blah' u'billion' u'vs' u'at' u'oct' u'reuter' u'pct' u'in'\n",
      " u'eight']\n",
      "\n",
      "Largest Coefs: \n",
      "[u'drug' u'market' u'said' u'the drug' u'a' u'aid' u'germani' u'test'\n",
      " u'german' u'that']\n"
     ]
    }
   ],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "print(len(feature_names))\n",
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "print(len(sorted_coef_index))\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1] \n",
    "# so the list returned is in order of largest to smallest\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression using Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382\n",
      "Smallest tfidf:\n",
      "[u'limited' u'set' u'yet' u'little' u'probably' u'until' u'less' u'already'\n",
      " u'half' u'where']\n",
      "\n",
      "Largest tfidf: \n",
      "[u'blah' u'loss' u'billion' u'the' u'oil' u'25' u'week' u'south' u'rate'\n",
      " u'vaccine']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Learn vocabulary and idf from training set.\n",
    "# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\n",
    "# When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. \n",
    "vectorizer = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "print(len(feature_names))\n",
    "sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "\n",
    "print('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382\n",
      "('AUC: ', 0.88888888888888884)\n"
     ]
    }
   ],
   "source": [
    "# Transform documents to document-term matrix. Returns a Tf-idf-weighted document-term matrix\n",
    "X_train_vectorized = vectorizer.transform(X_train)\n",
    "#print(X_train_vectorized)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vectorizer.transform(X_test))\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "[u'blah' u'billion' u'government' u'oct' u'19' u'oil' u'at' u'minister'\n",
      " u'year' u'statement']\n",
      "\n",
      "Largest Coefs: \n",
      "[u'drug' u'the' u'aids' u'fda' u'said' u'drugs' u'vaccine' u'of' u'that'\n",
      " u'to']\n"
     ]
    }
   ],
   "source": [
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression using CountVectorizer with n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the CountVectorizer to the training data specifiying a minimum \n",
    "# document frequency of 5 and extracting 1-grams and 2-grams\n",
    "vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AUC: ', 0.94444444444444442)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "[u'blah' u'blah blah' u'reuter' u'billion' u'19' u'will' u'april' u'oct'\n",
      " u'oct 19' u'at']\n",
      "\n",
      "Largest Coefs: \n",
      "[u'drug' u'german' u'products' u'germany' u'said' u'aids' u'had' u'to the'\n",
      " u'vaccine' u'said the']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaiveBayes using CountVectorizer with n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the CountVectorizer to the training data specifiying a minimum \n",
    "# document frequency of 5 and extracting 1-grams and 2-grams\n",
    "vectorizer = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "X_train_vectorized = vectorizer.transform(X_train)\n",
    "feats = vectorizer.get_feature_names()\n",
    "len(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf_nb = MultinomialNB(alpha=0.1)\n",
    "\n",
    "clf_nb.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive word -2.43: the\n",
      "Positive word -3.29: of\n",
      "Positive word -3.35: to\n",
      "Positive word -3.47: in\n",
      "Positive word -3.53: said\n",
      "Positive word -3.70: and\n",
      "Positive word -3.79: drug\n",
      "Positive word -4.13: for\n",
      "Positive word -4.21: that\n",
      "Positive word -4.43: it\n",
      "Positive word -4.57: is\n",
      "Positive word -4.64: of the\n",
      "Positive word -4.70: the drug\n",
      "Positive word -4.70: aids\n",
      "Positive word -4.71: in the\n",
      "Positive word -4.74: be\n",
      "Positive word -4.75: would\n",
      "Positive word -4.80: with\n",
      "Positive word -4.81: on\n",
      "Positive word -4.86: as\n",
      "Positive word -4.93: was\n",
      "Positive word -4.93: said the\n",
      "Positive word -4.96: by\n",
      "Positive word -5.00: new\n",
      "Positive word -5.00: fda\n"
     ]
    }
   ],
   "source": [
    "pf = [(clf_nb.feature_log_prob_[1, i], feats[i]) for i in range(len(feats))]\n",
    "pf.sort(reverse=True)\n",
    "for p in pf[:25]:\n",
    "    print 'Positive word %.2f: %s' % (p[0], p[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the text to arrays of numbers\n",
    "X_text_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<16x531 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 1147 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict the values of the test set\n",
    "predictions = clf_nb.predict(X_text_vectorized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, False, False, True, False, True, False, True, True, False, False, False, True, False, False]\n",
      "[True, True, False, False, True, False, True, False, True, True, False, False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "# Look at the first 20 predictions\n",
    "print(predictions.tolist())\n",
    "print(y_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def render_confusion_matrix(ytrue, ypred):\n",
    "    return pd.crosstab(pd.Series(ytrue), pd.Series(ypred), rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  False  True  All\n",
       "Actual                     \n",
       "False          9     1   10\n",
       "True           0     6    6\n",
       "All            9     7   16"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_test.tolist()\n",
    "render_confusion_matrix(y_test.tolist(), predictions.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:  0.9375\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      0.90      0.95        10\n",
      "       True       0.86      1.00      0.92         6\n",
      "\n",
      "avg / total       0.95      0.94      0.94        16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print\n",
    "print 'Accuracy: ', accuracy_score(y_test, predictions)\n",
    "print\n",
    "print classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iwrong_predictions = [i for i,v in enumerate(zip(y_test, predictions)) if v[0] != v[1]]\n",
    "\n",
    "iwrong_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "proba = clf_nb.predict_proba(X_text_vectorized)\n",
    "log_proba = clf_nb.predict_log_proba(X_text_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diff_prob = proba[:,1] - proba[:,0]\n",
    "diff_log_proba = log_proba[:,1] - log_proba[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff_prob:\n",
      "\n",
      "mean: -0.114946661721\n",
      "std: 0.981629220822\n",
      "\n",
      "diff_log_prob:\n",
      "\n",
      "mean: 50.5160502051\n",
      "std: 101.729117462\n"
     ]
    }
   ],
   "source": [
    "print 'diff_prob:\\n'\n",
    "print 'mean:',np.mean(diff_prob)\n",
    "print 'std:', np.std(diff_prob)\n",
    "\n",
    "print '\\ndiff_log_prob:\\n'\n",
    "print 'mean:', np.mean(diff_log_proba)\n",
    "print 'std:', np.std(diff_log_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x1f040390>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot  histogram.\n",
    "plt.hist(diff_log_proba, range=[-500, 500], bins=30, normed=True, alpha=0.5)\n",
    "plt.axvline(0, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
